{
    "title": "Reinforcement Learning for Topic Models",
    "authors": "Jeremy Costello and Marek Z. Reformat Department of Electrical and Computer Engineering University of Alberta {jeremy1, reformat}@ualberta.ca",
    "abstract": "We apply reinforcement learning techniques to topic modeling by replacing the variational autoencoder in ProdLDA with a continuous action space reinforcement learning policy. We train the system with a policy gradient algorithm REINFORCE. Additionally, we introduced several modi\ufb01cations: modernize the neural network architecture, weight the ELBO loss, use contextual embeddings, and monitor the learning process via computing topic diversity and coherence for each training step. Experiments are performed on 11 data sets. Our unsupervised model outperforms all other unsupervised models and performs on par with or better than most models using supervised labeling. Our model is outperformed on certain data sets by a model using supervised labeling and contrastive learning. We have also conducted an ablation study to provide empirical evidence of performance improvements from changes we made to ProdLDA and found that the reinforcement learning formulation boosts performance.",
    "sections": {
        "1": {
            "name": "Introduction",
            "text": "The internet contains large collections of unlabeled textual data. Topic modeling is a method to extract information from this text by grouping documents into topics and linking these topics with words describing them. Classical techniques for topic modeling, the most popular being Latent Dirichlet Approximation (LDA) (Blei et al., 2003), have recently begun to be overtaken by Neural Topic Models (NTM) (Zhao et al., 2021).\n\nProdLDA (Srivastava and Sutton, 2017) is a NTM using a product of experts in place of the mixture model used in classical LDA. ProdLDA uses a variational autoencoder (VAE) (Kingma and Welling, 2013) to learn distributions over topics and words. ProdLDA improved on NVDM (Miao et al., 2016) by explicitly approximating the Dirichlet prior from LDA with a Gaussian distribution and using the Adam optimizer (Kingma and Ba, 2014) with a higher momentum and learning rate. Perceiving Reinforcement Learning (RL) as probabilistic inference has brought practices of such an inference into the RL \ufb01eld (Dayan and Hinton, 1997) (Levine, 2018). New algorithms using these techniques include MPO (Abdolmaleki et al., 2018) and VIREL (Fellows et al., 2019). MPO optimizes the evidence lower bound (ELBO), which is the same optimization objective used in VAEs.\n\nInspired by the adoption of probabilistic inference techniques in RL, we look to apply RL techniques to probabilistic inference in the realm of topic models. We use REINFORCE, the simplest policy gradient (PG) algorithm, to train a model which parameterizes a continuous action space, corresponding to the distribution of topics for each document in the topic model. We keep the product of experts from ProdLDA to compute the distribution of words for each document in the topic model.\n\ntopic model by using Sentence-BERT (SBERT) embeddings (Reimers and Gurevych, 2019) rather than bag-ofword (BoW) embeddings, modernizing the neural network (NN) architecture, adding a weighting term to the ELBO, and tracking topic diversity and coherence metrics throughout training. The model architecture is shown in Figure 1. Our method outperforms most other topic models. It is beaten only on some data sets by advanced methods using document labels for supervised learning, while our procedure is fully unsupervised."
        },
        "2": {
            "name": "Related Work",
            "text": "Zhao et al. (2021) provide a survey of NTMs. Variations of VAEs are presented which use different distributions, correlated and structured topics, pretrained language models, incorporate meta-data, or model on short texts rather than documents. Methods other than VAEs are also used for NTMing, including autoregressive models, generative adversarial networks, and graph NNs.\n\nDoan and Hoang (2021) compare ProdLDA and NVDM, along with six other NTMs and three classical topic models, in terms of held-out document and word perplexity, downstream classi\ufb01cation, and coherence. Scholar (Card et al., 2017), an extension of ProdLDA taking document metadata and labels into account where possible, performed best in terms of coherence. NVDM and NVCTM (Liu et al., 2019), an extension of NVDM which additionally models the correlation between documents, performed best in terms of perplexity and downstream classi\ufb01cation. The other NTMs were GSM (Miao et al., 2017), NVLDA (Srivastava and Sutton, 2017), NSMDM (Lin et al., 2019), and NSMTM (Lin et al., 2019). The classical topic models were non-negative matrix factorization (NMF) (Zhao et al., 2017), online LDA (Hoffman et al., 2010), and Gibbs sampling LDA (Grif\ufb01ths and Steyvers, 2004).\n\nBERTopic (Grootendorst, 2022) and Top2Vec (Angelov, 2020) use dimensionality reduction and clustering to group document embeddings from pre-trained language models into meaningful clusters. Contextualized Topic Models (CTM) (Bianchi et al., 2020a) augments the BoW embeddings used in ProdLDA with SBERT (Reimers and Gurevych, 2019) embeddings, resulting in an improved topic model.\n\nDieng et al. (2020) develop the embedded topic model (ETM) by using word embeddings to augment a variational inference algorithm for topic modeling. Their method outperforms other topic models, especially on corpora with large vocabularies containing common and very rare words. Nguyen and Luu (2021) augment Scholar (Card et al., 2017) with contrastive learning (Hadsell et al., 2006) and outperform all topic models compared against.\n\nGui et al. (2019) use RL to \ufb01lter words from documents, with reward as a combination of the resulting topic model\u2019s coherence and diversity, or how few words overlap between topics. Kumar et al. (2021) use REINFORCE (Williams, 1992), a PG RL algorithm, to augment ProdLDA. Their model slightly outperforms ProdLDA in terms of topic coherence."
        },
        "3": {
            "name": "Background",
            "text": "We brie\ufb02y outline topic models, RL process, KL divergence, and contextual embeddings.\n\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003) is a three-level hierarchical Bayesian model: documents \u2192 topics \u2192 words. Each document is a mixture over latent topics, where the topic distribution \u03b8 is randomly sampled from a Dirichlet distribution. Each topic is a multinomial distribu tion over vocabulary words.\n\nAutoencoding Variational Inference for Topic Models (AVITM) (Srivastava and Sutton, 2017) is a neural topic model using a VAE to learn a Gaussian distribution over topics. VAEs use a reparameterization trick (RT) to randomly sample from the posterior distribution to remain fully differentiable. At the time, there was no known RT for Dirichlet distributions, so AVITM used a Gaussian distribution and a Laplace approximation of the Dirichlet prior.\n\nAVITM contains two models: NVLDA and ProdLDA. NVLDA uses the mixture model from LDA to infer a distribution over vocabulary words, while ProdLDA uses a product of experts.\n\nEvidence Lower Bound (ELBO) is the optimization objective for AVITM. ELBO optimization (Jordan et al., 1999) simultaneously tries to maximize the log-likelihood of the topic model and minimize the forward Kullback\u2013Leibler (KL) divergence (Kullback and Leibler, 1951) between the posterior P and prior Q topic distributions.\n\nELBO = DKL(P ||Q) \u2212 log-likelihood\n\nTopic Coherence is a metric for evaluating topic models. It uses co-occurence in a reference corpus to measure semantic similarity between the top-K words in a topic. Topic model coherence is the average of each topic\u2019s coherence.\n\ninformation (NPMI) (Aletras and Stevenson, 2013) was the coherence measure found to correlate best with human judgment (Lau et al., 2014). When computing NPMI, a window size of 20 for co-occurrence counts is used in Srivastava and Sutton (2017), while Dieng et al. (2020) uses full document cooccurrence.\n\nNPMI coherence is calculated for each of the topK words in a topic and averaged to obtain the coherence for that topic. The overall topic-coherence is the average of the coherence for each topic. For a word i, the NPMI coherence is calculated according to Equation 2.\n\nlog P (wi,wj ) P (wi)P (wj ) \u2212 log P (wi, wj) where P (wi) is the probability of word i occurring in a document in the corpus, and P (wi, wj) is the probability of words i and j co-occurring in a document in the corpus.\n\nTopic Diversity is another metric for evaluating topic models. It measures the uniqueness of the top-K words across all topics. Dieng et al. (2020) use K = 25 for reporting topic diversity.\n\nnumber-of -unique-words K \u2217 number-of -topics Topic Quality duced by Dieng et al. (2020).\n\nis a topic modeling metric intro topic-quality = topic-coherence \u2217 topic-diversity\n\nRL is a sequential decision-making framework focused on \ufb01nding the best sequence of actions executed by an agent. (Sutton and Barto, 2018). An agent takes actions a \u2208 A to traverse between states s \u2208 S in an environment, receiving a reward r on each transition. The goal of an RL task is to \ufb01nd the best set of actions \u2014referred to as the policy \u2014which maximizes the reward. RL problems can be episodic, where the agent completes the environment and is reset, or continuing, where the agent continuously traverses the environment without reset. Through traversing the environment, the agent learns a policy \u03c0 of which actions in each state will maximize return. Return is the cumulative reward received by the agent in an episode or its lifetime. It is usually discounted by a factor \u03b3 to favor near-term reward over long-term reward. An alternative to discounting is the average reward formulation.\n\nPolicy Gradient (PG) Algorithms Many RL algorithms learn a value function \u2013 representing values associated with selecting speci\ufb01c actions \u2013 and a corresponding policy that chooses the action or subsequent state with maximum value. PG algorithms (Sutton et al., 1999) provide an alternative approach directly learning a parameterized policy. The parameters of the policy function are optimized through stochastic gradient ascent.\n\nREINFORCE is a Monte Carlo PG algorithm for episodic problems (Williams, 1992). See algorithm 1, where \u03c1 is a vector of optimized parameters.\n\nInput: A differentiable parameterized policy function \u03c0(a|s, \u03c1) step size \u03b1 > 0, discount factor \u03b3 < 1\n\nGenerate an episode s0, a0, r1, . . . , sT \u22121, aT \u22121, rT following policy \u03c0 for each step in the episode (t from 0 to T \u2212 1) do G \u2190 (cid:80)T k=t+1 \u03b3k\u2212t\u22121rk \u03c1 \u2190 \u03c1 + \u03b1\u03b3tG\u2207 ln \u03c0(at|st, \u03c1) Continuous Action Spaces are one advantage of PG algorithms (Sutton and Barto, 2018). Parameterized policies allow action spaces that are parameterized by a probability distribution, such as a Gaussian. For Gaussian action spaces, the mean \u00b5 and standard deviation \u03c3 are given by function approximators parameterized by \u03c1. For a state s, an action a is sampled from the distribution and the policy is updated according to Equation 5.\n\nKullback-Leibler (KL) Divergence (Kullback and Leibler, 1951) measures the similarity between two probability distributions P and Q. It is used in AVITM (Srivastava and Sutton, 2017) to force the posterior distribution parameterized by the VAE to be the Laplace approximation of the Dirichlet prior. The KL divergence calculation for N topics is shown in Equation 6.\n\nDKL(P ||Q) =\n\nContextual embeddings dominate NLP tasks, replacing earlier methods, including Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., The BoW document representation used in ProdLDA is augmented with contextual embeddings from SBERT Bianchi et al. (2020a). They test three models: one with BoW, one with contextual embeddings, and one with both. They \ufb01nd that using both embeddings produces the best results, and the other two methods perform almost as well. One advantage of using solely contextual embeddings is that multilingual language models can encode documents from different languages into the same embedding space, enabling easy creation of multilingual topic models (Bianchi et al., 2020b).\n\nSentence-BERT is an extension of BERT using a Siamese network to extract semantically meaningful sentence embeddings (Reimers and Gurevych, 2019). In contrast to BERT, this allows SBERT embeddings to be compared using dot product or cosine similarity, making SBERT more suitable for tasks such as semantic similarity search and clustering."
        },
        "4": {
            "name": "Methodology",
            "text": "Following Liu et al. (2022), we contemporize the architecture of the inference network within ProdLDA. We replace the SoftPlus activation function (Glorot et al., 2011) with a GELU activation function (Hendrycks and Gimpel, 2016), replace batch normalization (Ioffe and Szegedy, 2015) with layer normalization (Ba et al., 2016), and replace all Xavier initialization (Glorot and Bengio, 2010) with \u03c1 \u223c N (0, 0.02).\n\nFor the inference network, we increase the number of units in each layer from 100 to 128, add weight decay of 0.01 to each layer, and place dropout layers (Srivastava et al., 2014) after each fully connected layer.\n\nWe replace the softmax activation after the topic distribution with an RL policy formulation (Equation 5). We use a training batch size of 1024. We clip all gradients to a maximum norm of 1.0 to prevent gradient explosion (Pascanu et al., 2013). Following Bianchi et al. (2020a), we set both distributional priors as trainable parameters. We lower optimizer learning rate to 3e-4 and momentum to 0.9.\n\nFollowing Bianchi et al. (2020a), we replace the BoW used by ProdLDA with contextualized embeddings from SBERT. We use the \"all-MiniLML6-v2\" model for encoding unpreprocessed documents as embedding vectors. BoW embeddings, used to calculate the log-likelihood of the topic model, are created using preprocessed documents.\n\nWe adopt the view of RL as a statistical inference method (Levine, 2018). The modernized inference network from ProdLDA is used to parameterize a continuous action space from which an action is sampled, and the policy is computed according to Equation 5. The topic model distribution over vocabulary words uses the product of experts from ProdLDA. We use REINFORCE to train the network, with a weighted version of ELBO as the reward. Each document embedding is a state in the environment, and each episode terminates after a single step (i.e., action). Each action is a sample from the topic distribution.\n\nFollowing Higgins et al. (2016), we allow modi\ufb01able relative entropy between the prior and posterior by weighting the KL divergence term in the ELBO. We de\ufb01ne a hyperparameter \u03bb as a multiplier on the KL divergence term.\n\nELBOweighted = \u03bbDKL(P ||Q) \u2212 log-likelihood (7)"
        },
        "5": {
            "name": "Results",
            "text": "We initially evaluate our topic model on the 20 Newsgroups data set with 20 topics. Results averaged over 30 random seeds are shown: loss in Figure 2, topic coherence in Figure 3, and topic diversity in Figure 4. Mean and 90% con\ufb01dence intervals are plotted. Topic diversity and coherence are calculated with K = 10. Documents are preprocessed following Bianchi et al. (2020a) with the additional step of removing all words with less than three letters. Models are trained for 1000 epochs with the AdamW optimizer (\u03b1 = 3e \u2212 4, \u03b21 = 0.9, \u03b22 = 0.999). We use \u03bb = 5, inference network dropout of 0.2, and no dropout after the RL policy (policy dropout). All other experiments use these same settings unless otherwise noted.\n\nWe compare our method to recent topic models found in the literature.\n\nIn the beginning, our approach is compared with all models evaluated by Doan and Hoang (2021). We use their preprocessed documents and replicate their results using K = 10 to calculate topic coherence. Following the authors, we sweep from 0.5*N topics to 3*N topics in intervals of 0.5*N (N being the \"correct\" number of topics for each data set). Next, we do a hyperparameter sweep over \u03bb of 1,\n\nCoherence Diversity Quality 0.\n\nWe compare results with the contrastive Scholar model from Nguyen and Luu (2021). For each data set we perform a hyperparameter search with 50 topics. Search ranges and best results for each data set are shown in Table 3. We use the best hyperparameters from this search for \ufb01nal training runs with 50 and 200 topics. We train for 2000 Paper PTHT best RL model (ours) PTHT best RL model (ours) PTHT best RL model (ours) PTHT best RL model (ours) PTHT best RL model (ours)\n\nTo provide empirical evidence that performance improvements come from the RL policy formulation, we do a study ablating relevant changes from NPMI Word2Vec 0.1823 0.2509 0.0280 0.1249 0.1207 0.3563 0.1008 0.3559 0.1300 0.\n\nthe \ufb01nal RL model down to the original ProdLDA model. All comparisons are performed on the 20 Newsgroups data set with 20 topics and use the same settings as subsection 5.1. Results are averaged over 30 random seeds and shown in Table 5."
        },
        "6": {
            "name": "Discussion",
            "text": "For the initial experiments on the 20 Newsgroups data set, the average loss (Figure 2) reaches a near plateau around the 200th epoch. Past this epoch, coherence (Figure 3) continues to increase slowly, and topic diversity (Figure 4) increases substantially until around the 400th epoch, past which it also continues to increase slowly. It shows that training beyond a plateau in loss can still improve NTM performance.\n\nCompared to Doan and Hoang (2021), the RL model performs on par with or better than other models across all four data sets, while the performance of other models varies greatly between data sets. On the Snippets, 20 Newsgroups, and W2Econtent data sets, the RL model with lower values of \u03bb usually performs better as the number of topics increases. However, it reverses on the W2E-title data set where \u03bb = 10 outperforms \u03bb = 1 on the two highest number of topics.\n\nThe RL model outperforms the Labeled ETM model from Dieng et al. (2020) in topic diversity, coherence, and quality. Furthermore, this compari\n\nContrastive Scholar RL model (ours) RL Policy Embedding SBERT SBERT BoW SBERT SBERT SBERT BoW IMDb Movie Reviews 200 Topics 50 Topics 0.188 0.197 0.199 0."
        },
        "7": {
            "name": "Conclusion",
            "text": "Inspired by the introduction of probabilistic inference techniques to RL, we take the approach to develop a NTM augmented with RL. Our model builds on the ProdLDA model, which uses a product of experts instead of the mixture model used in classical LDA. We improve ProdLDA by adding SBERT embeddings, an RL policy formulation, a weighted ELBO loss, and the improved NN architecture. In addition, we track topic diversity and coherence during a training process rather than only evaluating these metrics for the \ufb01nal model. Our fully unsupervised RL model outperforms most other topic models. It is only topped by contrastive Scholar \u2014a method using supervised labels during training \u2014in a few select cases."
        },
        "8": {
            "name": "Limitations",
            "text": "The main limitation identi\ufb01ed for our RL model is decreased performance as the vocabulary size increases. Our RL model also has a higher variance than some other topic models to which we compared. While our RL model performed well on all the data sets tested, this performance may not generalize to different data sets. The insights from the policy dropout sweep conducted may not apply to other topic models. The performance difference for NPMI coherence compared with Bianchi et al. (2020a) may be overstated since the model in that paper used a deprecated SBERT model that produces sentence embeddings of low quality1. For the comparison to Nguyen and Luu (2021), we used slightly different preprocessing than the authors. While the model can work on any languages with associated embedding models, all data sets used in this paper were in English. Our model has additional hyperparameters compared to some other models. So, it may require more tuning and, therefore, more GPU computing. The initial model was developed on a system with 8GB of RAM and a Nvidia GTX 1060 with 3GB of VRAM for a total of approximately 100 GPU hours. A single run of the model for 1000 epochs on this GPU requires less than an hour. Experiments using the New York Times data set were run on a system with 256GB of RAM and a Nvidia RTX 3090 for approximately 100 GPU hours. All other experiments were run on a system with 128GB of RAM and a Nvidia TITAN RTX for approximately 600 GPU hours."
        },
        "9": {
            "name": "Ethics Statement",
            "text": "All data sets used in this paper are cited. The New York Times data set2 is licensed under \"The New York Times Annotated Corpus Agreement\"3. The Tweets2011 corpus4 is available under the \"TREC 2011 Microblog Dataset Usage Agreement\"5 which additionally requires following the \"Twitter terms of service\"6. All other data sets are obtained from the recent literature. No sensitive information is used or inferred in this paper. The risk of harm times-annotated-corpus-ldc2008t19.pdf 4https://trec.nist.gov/data/tweets/ 5https://trec.nist.gov/data/tweets/tweets2011 from our model is low. Any artifacts in this paper are used following their intended use cases.\n\nWe would like to thank Federico Bianchi for assistance in \ufb01nding data sets. We would like to thank the creators and maintainers of Python and the following Python packages: lda, torch, numpy, biterm, scipy, gensim, tqdm, transformers, nltk, sentence_transformers, sklearn, and pandas. We would like to thank the following GitHub users for code inspiration: maifeng, smutahoang, dicegroup, shion-h, karpathy, estebandito22, akashgit, and MilaNLProc."
        },
        "10": {
            "name": "References",
            "text": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. 2018. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920.\n\nNikolaos Aletras and Mark Stevenson. 2013. Evaluating topic coherence using distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)\u2013Long Papers, pages 13\u201322.\n\nDimo Angelov. 2020. Top2vec: Distributed representations of topics. arXiv preprint arXiv:2008.09470.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E HinarXiv preprint ton. 2016. Layer normalization. arXiv:1607.06450.\n\nFederico Bianchi, Silvia Terragni, and Dirk Hovy. 2020a. Pre-training is a hot topic: Contextualized document embeddings improve topic coherence. arXiv preprint arXiv:2004.03974.\n\nFederico Bianchi, Silvia Terragni, Dirk Hovy, Debora Nozza, and Elisabetta Fersini. 2020b. Cross-lingual contextualized topic models with zero-shot learning. arXiv preprint arXiv:2004.07737.\n\nDavid M Blei and John D Lafferty. 2006. Dynamic In Proceedings of the 23rd internatopic models. tional conference on Machine learning, pages 113\u2013 120.\n\nDavid M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993\u20131022.\n\nDallas Card, Chenhao Tan, and Noah A Smith. 2017. Neural models for documents with metadata. arXiv preprint arXiv:1705.09296.\n\nPeter Dayan and Geoffrey E Hinton. 1997. Using expectation-maximization for reinforcement learning. Neural Computation, 9(2):271\u2013278.\n\nAdji B Dieng, Francisco JR Ruiz, and David M Blei. 2020. Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8:439\u2013453.\n\nThanh-Nam Doan and Tuan-Anh Hoang. 2021. Benchmarking neural topic models: An empirical study. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4363\u20134368.\n\nMatthew Fellows, Anuj Mahajan, Tim GJ Rudner, and Shimon Whiteson. 2019. Virel: A variational inference framework for reinforcement learning. Advances in neural information processing systems, 32.\n\nMikhail Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. Ad2018. vances in neural information processing systems, 31.\n\nSarah Filippi, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. 2010. Optimism in reinforcement learning and In 2010 48th Annual kullback-leibler divergence. Allerton Conference on Communication, Control, and Computing (Allerton), pages 115\u2013122. IEEE.\n\nXavier Glorot and Yoshua Bengio. 2010. Understanding the dif\ufb01culty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on arti\ufb01cial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings.\n\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse recti\ufb01er neural networks. In Proceedings of the fourteenth international conference on arti\ufb01cial intelligence and statistics, pages 315\u2013 323. JMLR Workshop and Conference Proceedings.\n\nThomas L Grif\ufb01ths and Mark Steyvers. 2004. Finding scienti\ufb01c topics. Proceedings of the National academy of Sciences, 101(suppl_1):5228\u20135235.\n\nMaarten Grootendorst. 2022. Bertopic: Neural topic modeling with a class-based tf-idf procedure. arXiv preprint arXiv:2203.05794.\n\nLin Gui, Jia Leng, Gabriele Pergola, Yu Zhou, Ruifeng Xu, and Yulan He. 2019. Neural topic model with reinforcement learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3478\u20133483.\n\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 2, pages 1735\u20131742. IEEE.\n\nDan Hendrycks and Kevin Gimpel. 2016. GausarXiv preprint sian error linear units (gelus). arXiv:1606.08415.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2016. beta-vae: Learning basic visual concepts with a constrained variational framework.\n\nTuan-Anh Hoang, Khoi Duy Vo, and Wolfgang Nejdl. 2018. W2e: a worldwide-event benchmark dataset In Proceedings for topic detection and tracking. of the 27th ACM International Conference on Information and Knowledge Management, pages 1847\u2013 1850.\n\nMatthew Hoffman, Francis Bach, and David Blei. 2010. Online learning for latent dirichlet allocation. advances in neural information processing systems, 23.\n\nSergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by In International reducing internal covariate shift. conference on machine learning, pages 448\u2013456. PMLR.\n\nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. 1999. An introduction to variational methods for graphical models. Machine learning, 37(2):183\u2013233.\n\nHilbert J Kappen, Vicen\u00e7 G\u00f3mez, and Manfred Opper. 2012. Optimal control as a graphical model inference problem. Machine learning, 87(2):159\u2013182.\n\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\nDiederik P Kingma and Max Welling. 2013. AutoarXiv preprint encoding variational bayes. arXiv:1312.6114.\n\nTaisuke Kobayashi. 2022. Optimistic reinforcement learning by forward kullback\u2013leibler divergence optimization. Neural Networks, 152:169\u2013180.\n\nSolomon Kullback and Richard A Leibler. 1951. On information and suf\ufb01ciency. The annals of mathematical statistics, 22(1):79\u201386.\n\nAmit Kumar, Nazanin Esmaili, and Massimo Piccardi. 2021. A reinforced variational autoencoder topic In International Conference on Neural Inmodel. formation Processing, pages 360\u2013369. Springer.\n\nKen Lang. 1995. Newsweeder: Learning to \ufb01lter In Machine Learning Proceedings 1995, netnews. pages 331\u2013339. Elsevier.\n\nJey Han Lau, David Newman, and Timothy Baldwin. 2014. Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530\u2013539.\n\nSergey Levine. 2018. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909.\n\nTianyi Lin, Zhiyue Hu, and Xin Guo. 2019. Sparsemax and relaxed wasserstein for topic sparsity. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 141\u2013 149.\n\nLuyang Liu, Heyan Huang, Yang Gao, Yongfeng Zhang, and Xiaochi Wei. 2019. Neural variational correlated topic modeling. In The World Wide Web Conference, pages 1142\u20131152.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. In Proceedings of the A convnet for the 2020s. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986.\n\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. In Learning word vectors for sentiment analysis. Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142\u2013150.\n\nRichard McCreadie, Ian Soboroff, Jimmy Lin, Craig Macdonald, Iadh Ounis, and Dean McCullough. 2012. On building a reusable twitter corpus. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 1113\u20131114.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.\n\nYishu Miao, Edward Grefenstette, and Phil Blunsom. 2017. Discovering discrete latent topics with neural In International Conference variational inference. on Machine Learning, pages 2410\u20132419. PMLR.\n\nYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural variational inference for text processing. In International conference on machine learning, pages 1727\u2013 1736. PMLR.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and JefEf\ufb01cient estimation of word arXiv preprint frey Dean. 2013. representations in vector space. arXiv:1301.3781.\n\nThong Nguyen and Anh Tuan Luu. 2021. Contrastive learning for neural topic model. Advances in Neural Information Processing Systems, 34:11974\u201311986.\n\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the dif\ufb01culty of training recurrent neural In International conference on machine networks. learning, pages 1310\u20131318. PMLR.\n\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.\n\nJipeng Qiang, Zhenyu Qian, Yun Li, Yunhao Yuan, and Xindong Wu. 2020. Short text topic modeling techniques, applications, and performance: a survey. IEEE Transactions on Knowledge and Data Engineering, 34(3):1427\u20131445.\n\nNils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\n\nEvan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR.\n\nAkash Srivastava and Charles Sutton. 2017. Autoencoding variational inference for topic models. arXiv preprint arXiv:1703.01488.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from over\ufb01tting. The journal of machine learning research, 15(1):1929\u20131958.\n\nRichard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.\n\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12.\n\nYuan Tian, Minghao Han, Chetan Kulkarni, and Olga Fink. 2022. A prescriptive dirichlet power allocation policy with deep reinforcement learning. Reliability Engineering & System Safety, 224:108529.\n\nNaonori Ueda and Kazumi Saito. 2002. Parametric mixture models for multi-labeled text. Advances in neural information processing systems, 15.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\n\nNino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R\u00e9mi Munos, and Matthieu Geist. 2020. Leverage the average: an analysis of kl regularization in reinforcement learning. Advances in Neural Information Processing Systems, 33:12163\u2013 12174.\n\nRonald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3):229\u2013256.\n\nHe Zhao, Dinh Phung, Viet Huynh, Yuan Jin, Lan Du, and Wray Buntine. 2021. Topic modelling meets deep neural networks: A survey. arXiv preprint arXiv:2103.00498.\n\nRenbo Zhao, Vincent Tan, and Huan Xu. 2017. Online nonnegative matrix factorization with general divergences. In Arti\ufb01cial Intelligence and Statistics, pages 37\u201345. PMLR.\n\nA KL Divergence in RL KL divergence has recently become popular in continuous action space RL algorithms. One application is to prevent policy updates from making large changes to the policy that could result in poorer performance. Two algorithms using KL divergence for this are TRPO (Schulman et al., 2015) and MPO (Abdolmaleki et al., 2018). Another application is for optimistic RL (Filippi et al., 2010) (Kobayashi, 2022). Vieillard et al. (2020) investigate the usage of KL divergence as regularization in RL. KL divergence has also been used in optimal control (Kappen et al., 2012), which is closely related to RL.\n\nB Data Sets We evaluate models on the test set where available, and on the training set if there is no test set. Coherence and diversity for the training and test set are the same, as they are evaluated on the word distribution over topics which doesn\u2019t change per document. In the code, training coherence and diversity are computed after each batch, while test coherence and diversity are computed after each epoch. Number of training/test documents and vocabulary sizes are shown in Table 6. Average original and preprocessed training document lengths are shown in Table 7.\n\nThe 20 Newsgroups data set (Lang, 1995) consists of around 19,000 newsgroup posts from 20 topics. We perform experiments on this data set with three different preprocessing methods. For our initial experiments, we follow the preprocessing in Bianchi et al. (2020a) and additionally remove all words with less than 3 letters. For the comparisons with Bianchi et al. (2020a) and Nguyen and Luu (2021), we follow the preprocessing in Bianchi et al. (2020a). For the comparison with Doan and Hoang (2021), we use their already preprocessed data set.\n\nB.2 New York Times The New York Times data set (Sandhaus, 2008) consists of over 1.8 million articles written by the New York Times between 1987 and 2007. We follow the preprocessing from Bianchi et al. (2020a), but do not remove stopwords.\n\nThe Web Snippets data set (Ueda and Saito, 2002) consists of around 12,000 snippets of text from websites linked on \"yahoo.com\". The snippets are grouped into 8 domains. We use the already preprocessed data set from Doan and Hoang (2021).\n\nThe W2E data set (Hoang et al., 2018) consists of news articles from media channels around the world. The W2E-title subset is the titles from the news articles, while the W2E-content subset is the text content of the articles. The articles are grouped into 30 topics. We use the already preprocessed data set from Doan and Hoang (2021).\n\nThe Wiki20K data set (Bianchi et al., 2020b) consists of 20,000 English Wikipedia abstracts randomly sampled from DBpedia. We follow the preprocessing from Bianchi et al. (2020a).\n\nThe StackOver\ufb02ow data set (Qiang et al., 2020) consists of around 16,000 question titles randomly sampled from 20 different tags in a larger data set crawled from the website \"stackover\ufb02ow.com\" between July and August 2012. We use the already preprocessed data set from Qiang et al. (2020).\n\nB.7 Google News The Google News data set (Qiang et al., 2020) consists of around 11,000 titles and short samples from Google News articles clustered into 152 groups. We use the already preprocessed data set from Qiang et al. (2020).\n\nThe Tweets2011 data set (Qiang et al., 2020) consists of around 2,500 tweets in 89 clusters sampled from the larger Tweets2011 corpus (McCreadie et al., 2012) crawled from Twitter between January and February 2011. We use the already preprocessed data set from Qiang et al. (2020).\n\nComparison Paper This one (Bianchi et al., 2020a) (Nguyen and Luu, 2021) (Doan and Hoang, 2021) (Dieng et al., 2020) (Doan and Hoang, 2021) (Doan and Hoang, 2021) (Doan and Hoang, 2021) (Bianchi et al., 2020a) (Bianchi et al., 2020a) (Bianchi et al., 2020a) (Bianchi et al., 2020a) (Nguyen and Luu, 2021) (Nguyen and Luu, 2021)\n\nNew York Times Snippets W2E-title W2E-content Wiki20K StackOver\ufb02ow Google News Tweets2011 IMDb Movie Reviews Wikitext-103 IMDb Movie Reviews The IMDb Movie Reviews data set (Maas et al., 2011) consists of 50,000 movie reviews, each with an associated sentiment label, from the website \"imdb.com\". We follow the preprocessing from Bianchi et al. (2020a).\n\nThe Wikitext-103 data set (Merity et al., 2016) consists of around 28,500 Wikipedia articles classi\ufb01ed as either Featured articles or Good articles by Wikipedia editors. We follow the preprocessing from Bianchi et al. (2020a).\n\nC Evaluation Metrics We track topic diversity, coherence, perplexity, and loss for the training and test sets if applicable. Topic diversity and coherence are calculated based on the top-K words in each topic, with K noted for each experiment. We use NPMI coherence with co-occurence based on full document windows.\n\nMost previous NTMs have only reported the coherence of the \ufb01nal model, presumably because coherence is not tracked during training for computational reasons. To enable tracking of coherence during training, we modify a vectorized implementation of UMass coherence7 to calculate NPMI coherence and add caching for further speed-up. We also implement a GPU-optimized algorithm to calculate topic diversity during training.\n\nTraining Docs Test Docs Vocab Size Tracking these metrics during training provides two main bene\ufb01ts. The \ufb01rst bene\ufb01t is that if training is going poorly, it can be terminated. Poor training could be caused by component collapse (low topic diversity), or if the model is unable to \ufb01t to coherent topics (low coherence). The second bene\ufb01t is enabling deeper performance comparisons between models and between training runs for a single model. Most existing NTMs only track loss and perplexity during training, so additionally tracking topic diversity and coherence could provide additional insights on model performance.\n\nD Expanded Results D.1 Topic Words from Initial Experiments We choose one example of the top 10 words for all 20 topics from the initial experiments on the 20 Newsgroups data set. We choose the seed with the 15th highest coherence (out of 30 seeds). Topic words are shown in Table 8. Each document in the Twenty Newsgroups data set is labeled as belonging to one of 20 categories. These 20 categories are shown in Table 9.\n\nD.2 Pre-training is a Hot Topic We show a further comparison between the contextual embedding model from Bianchi et al. (2020a) and our RL model in Table 10. Average NPMI coherence over 30 seeds is compared for each number of topics: 25, 50, 75, 100, and 150.\n\nThis one (Bianchi et al., 2020a) (Nguyen and Luu, 2021) (Doan and Hoang, 2021) (Dieng et al., 2020) (Doan and Hoang, 2021) (Doan and Hoang, 2021) (Doan and Hoang, 2021) (Bianchi et al., 2020a) (Bianchi et al., 2020a) (Bianchi et al., 2020a) (Bianchi et al., 2020a) (Nguyen and Luu, 2021) (Nguyen and Luu, 2021)\n\nNew York Times Snippets W2E-title W2E-content Wiki20K StackOver\ufb02ow Google News Tweets2011 IMDb Movie Reviews Wikitext-103 We show the hyperparameters for each experiment we performed. Experiment seeds are generated with a meta-seed for reproducibility. The metaseed is randomly chosen from integers between 0 and 232. Values in {curly brackets} indicate a search over multiple parameters. Values in [square brackets] indicate NN layer sizes (e.g. [128, 128] represents two layers of size 128).\n\nInitial Experiments and Ablation Study We use the same meta-seed for the ablation study as we did for the initial experiments. Hyperparameters for the initial experiments can be found in Table 11. Further tables for all experiments will only show hyperparameters that differ from this table. Hyperparameters for the ablation study can be found in Table 12.\n\nD.3.2 Benchmarking Neural Topic Models We show hyperparameters for the comparison with Doan and Hoang (2021). Hyperparameters for Snippets can be found in Table 15. 20 Newsgroups in Table 16. W2E-title in Table 17. W2E-content in Table 19.\n\nD.3.3 Topic Modeling in Embedding Spaces Hyperparameters for the comparison with Dieng et al. (2020) can be found in Table 20.\n\nAverage Training Document Length Preprocessed 95.\n\nD.3.4 Pre-training is a Hot Topic We show hyperparameters for the comparison with Bianchi et al. (2020a). Data set and seed information can be found in Table 13. All other hyperparameters are the same for each data set; these can be found in Table 18.\n\nD.3.5 Contrastive Learning for NTM We show hyperparameters for the comparison with Nguyen and Luu (2021). Some hyperparameters are already shown in Table 3 and won\u2019t be shown again here. Data set and seed information can be found in Table 14. Other hyperparameters are the same for each data set; these can be found in Table 21. Hyperparameters for the policy dropout sweep can be found in Table 22.\n\nD.4 Ablation Study We show full results from the ablation study in Table 23.\n\nE Model Parameter Count The number of parameters (P) in the model differs based on the total number of parameters across all inference layers (L), the number of topics (N), and the vocabulary size (V). Trainable parameters are the inference layers, the prior distribution of topics (N x 1), and the distribution of words over topics (V * N). Total parameters can be calculated with Equation 8.\n\nP = L + N + V \u2217 N Topic Words max giz bhj chz pts buf air det pit bos morality objective cramer moral livesey optilink keith homosexual clayton gay window xterm widget lib windows font usr mouse motif application gun guns militia \ufb01rearms weapons cops weapon amendment semi arms team players hockey game season nhl games play teams leafs max giz bhj sale chz shipping offer monitor copies condition jesus god bible christ christians faith church christian heaven lord geb banks msg patients gordon pitt disease pain doctor medical fbi batf koresh compound atf waco sandvik udel \ufb01re kent car insurance cars dealer oil saturn honda engine bmw miles jpeg image bits display gif \ufb01le program \ufb01les format color clipper encryption key chip escrow keys privacy crypto secure nsa wire ground circuit connected cable atheism electrical universe keyboard output israel israeli arab jews arabs peace palestinian attacks bony villages turkish armenian armenians armenia turks serdar argic turkey genocide soviet pub ftp anonymous tar graphics privacy mailing archive motif faq moon space lunar orbit nasa spacecraft henry launch shuttle solar dog bike dod riding ride motorcycle rider bmw went cops scsi ide drive controller drives bus disk \ufb02oppy bios isa stephanopoulos president jobs myers russia russian administration package launch clinton Category alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc The largest model we use is for the Wikitext-103 data set with 200 topics. This model has 4,001,224 parameters.\n\nF Future Work We have identi\ufb01ed some possible paths for future work. The SBERT embeddings could be \ufb01ne-tuned during training rather than calculating them during pre-processing and freezing them during training. The RL formulation of our model could be extended to dynamic topic models (Blei and Lafferty, 2006). More complex PG RL algorithms could be used rather than REINFORCE, or a baseline could be added to REINFORCE. Exploration techniques from RL could be applied. The in\ufb02uence of hyperparameters (e.g. inference network layer sizes) on varied corpora (e.g. those with large vocabularies) could be explored. The Laplace approximation of the Dirichlet prior could be replaced by a true Dirichlet prior, making use of the Dirichlet RT (Figurnov et al., 2018) and a Dirichlet RL policy (Tian et al., 2022). Finally, \u03bb and the policy dropout could be scheduled during training to provide an automated tradeoff between topic diversity and coherence.\n\nPTHT RL model (ours) PTHT RL model (ours) PTHT RL model (ours) PTHT RL model (ours) PTHT RL model (ours)\n\nValue(s) 4174224060 30 1000 20 Newsgroups 2000 SBERT 20 0.2 0.0 [128, 128] GELU \u03c1 \u223c N (0, 0.02) Layer 5 10 (cid:88) \u00d7 3e-4 0.9, 0.999 0.01 1024 1.\n\nValue(s) 4174224060 30 20 Newsgroups {BoW, SBERT} {0.0, 0.2} {1, 5} {(cid:88), \u00d7} {(cid:88), \u00d7} Vocab Size Meta-seed 359491602 1459046441 925040003 1321150024 3277797161\n\nVocab Size Meta-seed 1553571489 3747305026 2672751736\n\nValue(s) 193270011 10 Snippets 4666 {4, 8, 12, 16, 20, 24} {1, 3, 5, 10} Value(s) 1216545997 10 20 Newsgroups 4157 {10, 20, 30, 40, 50, 60} {1, 3, 5, 10} Value(s) 4014169843 10 W2E-title 3703 {15, 30, 45, 60, 75, 90} {1, 3, 5, 10} Value(s) {25, 50, 75, 100, 150} 1 Num. Seeds 30 30 30 Value(s) 1359128464 10 W2E-content 10508 {15, 30, 45, 60, 75, 90} {1, 3, 5, 10} Value(s) 2337766308 1 20 New York Times 10283 300 0.5 [512, 512] 1 10* 32768 Value(s) 2000 {50, 200} Value(s) 3432645033 30 20 Newsgroups 2000 50 0.5 {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9} [128, 128] 1 RL Policy Embedding BoW BoW BoW BoW BoW BoW BoW BoW BoW BoW BoW BoW BoW BoW BoW BoW SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT SBERT Coherence Diversity 0.8457 0.6943 0.8905 0.8707 0.6598 0.6928 0.5965 0.9403 0.7390 0.5195 0.5692 0.7740 0.6222 0.5768 0.314 0.8092 0.6207 0.5995 0.8080 0.9070 0.4458 0.4530 0.6927 0.9530 0.6063 0.5430 0.5520 0.7663 0.4478 0.3698 0.9227 0.\n\n"
        }
    }
}